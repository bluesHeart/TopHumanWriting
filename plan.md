# 计划：把 AI Word Detector 升级为“对齐范文”的本地写作产品（8GB Windows / 50–100 PDF / 中英混合）

## 0. 一句话愿景
用户用“本地论文库（范文）”建索引；对选中段落一键获取**检索到的范文对照 + 带出处引用 + 受控改写（轻改/中改）**，尽可能“像范文写法”，并尽可能抑制 LLM 发散。

---

## 1. 约束与明确假设（按你的输入固定）
- 平台：Windows 笔记本，内存 8GB（CPU-only 作为默认场景）。
- 语料：50–100 篇 PDF，中文/英文/混合都可能出现。
- 交互：以“用户选中句子/段落”为主；不做整篇自动重写（后续可扩展）。
- 风格目标：**对齐（模仿范文写法）**，优先“同义改写 + 句式对齐 + 学术口吻”，尽量避免新观点/新事实/新引用。
- 分发：可打包“解压即用”的本地网页包（自带 python + 依赖 + embedding/语法模型）；润色生成改为走 **OpenAI-compatible 大模型 API**（不再随包附带本地大模型，减轻体积与维护成本）。

---

## 2. 产品范围：MVP vs. 后续

### 2.1 MVP（必须做）
1) **本地论文库建库**：PDF 文件夹 → 按页抽取/清洗 → chunk → 向量索引（可增量、可取消、可恢复）。
2) **范文对照检索**：对选中段落检索 top-k 范文片段；展示 `PDF 路径 + 页码 + 片段原文`；可复制/打开来源。
3) **对齐润色（受控）**：
   - 输出两档：轻改 / 中改（默认轻改更保守）。
   - 每条改写都带“为什么这样改”（基于范文片段证据），并严格绑定引用。
4) **一键应用/撤销**：diff 预览 → 应用到输入框 → 可撤销。
5) **可打包本地**：解压即用的本地网页包（自带 python + 依赖 + embedding/语法模型；润色生成走 API）。

### 2.2 后续（不阻塞 MVP）
- 整篇/章节级润色工作流（计划任务队列、批处理、导出报告）。
- DOCX/Markdown 导入导出（保留格式、批注）。
- 多风格 Profile（不同会议/期刊写法、不同导师偏好）。
- 自动抽取“范文 style guide”（句式、常用结构、连接词分布）。
- 评测：内置基准集、对齐度/保真度打分、回归测试。

---

## 3. 技术选型（与 8GB/离线/打包对齐）

### 3.1 编排框架：LlamaIndex
用途：ingestion（构建 Node/metadata）、检索编排、citation 输出结构。

建议采用模块化依赖（减少体积与冲突）：
- `llama-index-core`
- `llama-index-vector-stores-faiss`（默认）
- `llama-index-vector-stores-chroma`（可选）

### 3.2 向量库：FAISS 默认，Chroma 可选
- **FAISS（默认）**：Windows wheel 可用、依赖轻、性能好。
- **Chroma（可选）**：持久化/元数据友好，但依赖更重；在 8GB 上不作为默认路径。

### 3.3 大模型：OpenAI-compatible API（云端/私有部署）
关键点：
- 统一走 OpenAI-compatible ChatCompletions 接口（通常是 `/v1/chat/completions`），方便切换供应商与网关。
- 默认 `temperature=0` + 严格 JSON schema + 校验/修复重试，尽量把“改写”约束成可审计的白箱流程（而不是自由发挥）。
- 失败回退：仍可提供“范文对照 + 可复制的句式模板”，不中断工作流。

### 3.4 Embedding：优先复用现有 ONNX（已在项目中）
现有语义模型（`models/semantic/`）已经支持中英句向量，建议复用：
- 统一 embedding 逻辑：既用于“异常检测/范句”，也用于“RAG 索引/检索”与“改写保真度约束”。
- 减少新模型依赖与下载复杂度。

---

## 4. 核心机制：如何“对齐”且“抑制发散”

### 4.1 生成策略（默认安全配置）
- `temperature=0`（固定），优先“句式对齐/受控改写”而非自由发挥；必要时再放开到 0.1。
- `max_tokens` 默认给足（建议 2048–4096+），避免 JSON 被截断；仍设置上限防止失控输出。
- 输入长度硬上限：选中段落过长时先提示切段或自动切段（避免 prompt 过长）。

### 4.2 输出结构化 + 校验（强制）
LLM 必须输出严格 JSON（示例字段）：
```json
{
  "language": "zh|en|mixed",
  "variants": [
    {
      "level": "light|medium",
      "rewrite": "...",
      "changes": ["..."],
      "citations": [{"id":"C3","pdf":"...","page":12,"quote":"..."}]
    }
  ]
}
```
校验规则（不通过则自动要求“修复到合规”，否则回退到“无 LLM 建议”）：
- 引用 `id` 必须来自检索提供的 `C1..Ck`。
- `quote` 必须是对应 chunk 原文的子串。
- 禁止新增：数字/年份、括号引用（如 `[1]`、`(Smith, 2020)`）、新专有名词（简单规则：英文大写串/疑似人名机构模式等）。
- 保真度：改写与原文 embedding 相似度低于阈值则判失败（阈值可调）。

### 4.3 回退路径（8GB 必须有）
当 LLM 未启动/内存不足/输出不合规：
- 仍提供“范文对照 + 可编辑建议”（不依赖 LLM）。

---

## 5. 数据与持久化设计（按 Library 分隔，可增量）

### 5.1 建议目录结构（新）
默认仍遵循 `AIWORDDETECTOR_DATA_DIR`（便携/可清理）：
```
AIWordDetector_data/
  settings.json
  libraries/                  # 现有：统计与语义离群索引
    <name>.json
    <name>.sentences.json
    <name>.embeddings.npy
  rag/                        # 新增：RAG 索引与清单（每个库一套）
    <name>/
      manifest.json           # PDF 列表 + mtime/size/hash + 建库版本
      nodes.jsonl             # chunk 文本 + metadata（调试/回滚友好）
      faiss.index             # FAISS 向量索引（默认）
      storage/                # LlamaIndex storage（docstore/index store）
models/
  semantic/                   # 现有：离线语义模型（ONNX）
  syntax/                     # 现有：UDPipe 模型（可选）
```

### 5.2 增量更新策略
- `manifest.json` 记录每个 PDF 的 `path_rel, size, mtime, quick_hash`。
- 建库时只重算变化文件对应的 chunk，并更新索引（FAISS 可重建或增量；MVP 可先“变化即重建”，后续再做增量写入）。
- 建库全程可取消；采用“临时文件 + 原子替换（os.replace）”避免半成品破坏旧库。

---

## 6. API 与 8GB 预算（默认推荐值）

### 6.1 模型建议（中英混合 + 稳定 JSON）
- 推荐优先选择“响应快、指令遵循好、输出稳定”的模型；如果更偏重“句式模仿”，可在同一网关下尝试不同模型对比。
- 默认策略：`temperature=0` + `max_tokens` 足够大（建议 4096+）+ 校验失败自动修复重试（最多 2–3 次）。

### 6.2 8GB 场景的性能策略（前端/后端）
- 检索侧（本地）：限制 top-k（默认 6–8）+ 限制每条范文片段长度（例如 500–800 字符）以控制 prompt 体积。
- 生成侧（API）：失败优先提示“输出长度不足导致 JSON 截断”，引导用户把 `max_tokens` 调大，而不是让模型自由发挥。
- 回退侧：API 不可用时仍可做“范文对照 + 可复制句式模板”，不中断写作。

---

## 7. 详细里程碑（按可交付/可验收拆解）

> 每个里程碑都要求：可运行、可回滚、可验证；不破坏现有“怪异度检测”主流程。

### M0：工程化准备（0.5–1 天）
- 新增模块目录（建议）：
  - `rag/`：ingestion、chunker、index、retriever
  - `llm_api/`：OpenAI-compatible 客户端、配置、重试、JSON 校验/修复
  - `polish/`：对齐建议、diff/apply/undo
- 配置项落盘（settings.json）：RAG 参数、API base_url/model（api_key 建议走环境变量）、后端选择（faiss/chroma）。
- 依赖梳理：最小依赖集合 + 可选依赖集合。

验收：
- 程序可启动；新配置可保存；不影响现有分析功能。

### M1：PDF ingestion → chunk → nodes（1–2 天）
- 按页抽取文本（复用 PyMuPDF）。
- 清洗：页眉页脚、软换行、目录/参考文献降权或剔除。
- Chunk：段落/句组（中英分别断句，混合按段落优先）。
- 写出 `nodes.jsonl` 与 `manifest.json`（支持取消/原子替换）。

验收：
- 50–100 PDF 可建库；随机抽 10 条 node 可定位到 pdf+page 且文本干净。

### M2：Embedding + FAISS 索引与持久化（1–2 天）
- 复用现有 ONNX embedder 生成向量（批量、节流 UI 更新）。
- FAISS 建索引并落盘；LlamaIndex StorageContext 持久化（或先自管元数据，后续再深度整合）。
- 检索 API：输入 query → 输出 `[(score, chunk, pdf, page)]`。

验收：
- 选中段落 → 1 秒级返回 top-k（库规模约 50–100 PDF）。
- 引用信息稳定可用（路径/页码/片段）。

### M3：UI 集成“范文对照”（1–3 天）
- 新增面板：`范文对照`（列表 + 展开详情 + 复制/打开）。
- 右键菜单：`对齐范文（仅对照）`、`对齐范文（建议）`、`对齐范文（轻改）`、`对齐范文（中改）`。
- 长任务进度复用现有进度面板（阶段：extract → embed → index）。

验收：
- 用户无须启动 LLM 也能用“对照+建议”完成改写。

### M4：无 LLM 的对齐建议（2–4 天）
- 规则/统计建议（不改写原文，只给可编辑建议）：
  - 连接词、学术口吻替换建议（中英）
  - 句长对齐建议（基于范文片段与库基线）
  - 术语一致性提醒（从范文片段抽取高频术语）
- UI：建议可复制、可插入、可一键应用局部替换。

验收：
- 不启 LLM 仍有“可落地”的改写路径；建议能明显更像范文。

### M5：API 设置与连通性测试（0.5–1 天）
- 增加 API 设置页：base_url / model / api_key（支持 env 优先；可选写入 settings.json）。
- 一键测试：发出一次严格 JSON 的最小请求，显示 http 状态、错误摘要与当前配置来源（env / settings）。
- 可观测性：把最近一次 API 错误写入 logs/（便于定位 key/网关/网络问题）。

验收：
- 配置后“一键测试”通过；失败时提示明确（401/403/429/超时/接口不兼容等）。

### M6：受控改写（对齐润色）+ 校验回退（3–6 天）
- Prompt：输入=原文段落 + top-k 范文片段（带 `C1..Ck`）+ 禁止项；输出严格 JSON。
- 校验：结构/引用/禁止项/保真度；失败自动请求“修复”；仍失败则回退 M4。
- UI：diff 预览、应用/撤销、版本对比（轻改 vs 中改）。

验收：
- 输出稳定合规；不合规不会污染 UI（一定回退）。
- 改写不引入新事实/新引用（规则检测通过率高）。

### M7：发布（Web 离线包）（1–2 天）
- 放弃桌面 exe，改为“解压即用的本地网页”。
- 提供 `build_release_web.bat`：生成 `release/TopHumanWriting_<version>_offline.zip`。
- 离线包内容：`webapp/` + `aiwd/` + `models/` + `run_web.bat` + `setup_env.bat` + `requirements.txt`。
- 首次运行检测：在网页的 **API 设置** 页明确显示 base_url/model/key 是否配置，并提供“一键测试”。

验收：
- 解压后双击 `run_web.bat` → 浏览器打开 → “建库→对齐扫描→对齐润色”可跑通。

---

## 8. 风险清单与应对
- **PDF 噪声影响检索**：页眉页脚/参考文献/表格必须清洗或降权；否则“对齐”会学到坏风格。
- **API 不可用/配置错误**：未配置 key、401/403/429、网关不兼容接口、网络波动；需提供明确报错 + 可用回退（只看范文证据）。
- **对齐质量不稳定**：引入严格校验与重试；提示词“只改写法不改事实”；轻改优先。

---

## 9. 推荐的先做顺序（最短路径）
1) M1+M2：先把“可引用的范文检索”做出来（不用 LLM 也有价值）。
2) M3+M4：把“对照 + 无 LLM 建议”做成可用产品。
3) M5+M6：再上受控改写（LLM），用校验与回退保证稳定。
4) M7：最后做本地网页打包与首次运行引导。

---

## 10. 待确认（实现前最后的“可选项”）
- 默认 API 提供方/网关（base_url）与模型名（model）。
- 是否允许把 `api_key` 写入 settings.json（默认不建议，优先走环境变量）。
- 是否需要“打开 PDF 并定位页码”的深度集成（简单版：打开文件；高级版：定位页码/高亮需要额外工作）。
- 是否将 RAG 索引与现有 library 统计合并管理（同名库一键建两套索引）。
